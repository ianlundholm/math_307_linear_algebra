
\section*{Exercises for \ref{sec:8_2}}

\begin{Filesave}{solutions}
\solsection{Section~\ref{sec:8_2}}
\end{Filesave}

\begin{multicols}{2}
\begin{ex}
Normalize the rows to make each of the following matrices orthogonal.

%[label={\alph*.}]
\begin{exenumerate}[column-sep=-5em]
\exitem $A = \leftB \begin{array}{rr}
1 & 1 \\
-1 & 1
\end{array}\rightB$
\exitem $A = \leftB \begin{array}{rr}
3 & -4 \\
4 & 3
\end{array}\rightB$
\exitem* $A = \leftB \begin{array}{rr}
1 & 2 \\
-4 & 2
\end{array}\rightB$
\exitem* $A = \leftB \begin{array}{rr}
a & b \\
-b & a
\end{array}\rightB$, $(a,b) \neq (0,0)$
\exitem* $A = \leftB \begin{array}{ccc}
\cos\theta & -\sin\theta & 0 \\
\sin\theta & \cos\theta & 0 \\
0 & 0 & 2 
\end{array}\rightB$
\exitem* $A = \leftB \begin{array}{rrr}
2 & 1 & -1 \\
1 & -1 & 1 \\
0 & 1 & 1 
\end{array}\rightB$
\exitem* $A = \leftB \begin{array}{rrr}
-1 & 2 & 2 \\
2 & -1 & 2 \\
2 & 2 & -1 
\end{array}\rightB$
\exitem* $A = \leftB \begin{array}{rrr}
2 & 6 & -3 \\
3 & 2 & 6 \\
-6 & 3 & 2 
\end{array}\rightB$
\end{exenumerate}
\begin{sol}
\begin{enumerate}[label={\alph*.}]
\setcounter{enumi}{1}
\item  $\frac{1}{5}\leftB \begin{array}{rr}
3 & -4 \\
4 & 3
\end{array}\rightB$

\setcounter{enumi}{3}
\item  $\frac{1}{\sqrt{a^2 + b^2}}\leftB \begin{array}{rr}
a & b \\
-b & a
\end{array}\rightB$

\setcounter{enumi}{5}
\item  $\leftB \begin{array}{rrr}
\frac{2}{\sqrt{6}} & \frac{1}{\sqrt{6}} & -\frac{1}{\sqrt{6}}\\
\frac{1}{\sqrt{3}} & -\frac{1}{\sqrt{3}} & \frac{1}{\sqrt{3}} \\
0 & \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}
\end{array}\rightB$


\setcounter{enumi}{7}
\item  $\frac{1}{7}\leftB \begin{array}{rrr}
2 & 6 & -3 \\
3 & 2 & 6 \\
-6 & 3 & 2
\end{array}\rightB$

\end{enumerate}
\end{sol}
\end{ex}

\begin{ex}
If $P$ is a triangular orthogonal matrix, show that $P$ is diagonal and that all diagonal entries are $1$ or $-1$.

\begin{sol}
We have $P^{T} = P^{-1}$; this matrix is lower triangular (left side) and also upper triangular (right side--see Lemma~\ref{lem:006547}), and so is diagonal. But then $P = P^{T} = P^{-1}$, so $P^{2} = I$. This implies that the diagonal entries of $P$ are all $\pm 1$.
\end{sol}
\end{ex}

\begin{ex}
If $P$ is orthogonal, show that $kP$ is orthogonal if and only if $k = 1$ or $k = -1$.
\end{ex}

\begin{ex}
If the first two rows of an orthogonal matrix are $(\frac{1}{3}, \frac{2}{3}, \frac{2}{3})$ and $(\frac{2}{3}, \frac{1}{3}, \frac{-2}{3})$, find all possible third rows.
\end{ex}

\begin{ex}
For each matrix $A$, find an orthogonal matrix $P$ such that $P^{-1}AP$ is diagonal.

\begin{exenumerate}[column-sep=-15pt]
\exitem $A = \leftB \begin{array}{rr}
0 & 1 \\
1 & 0
\end{array}\rightB$
\exitem $A = \leftB \begin{array}{rr}
1 & -1 \\
-1 & 1
\end{array}\rightB$
\exitem $A = \leftB \begin{array}{rrr}
3 & 0 & 0 \\
0 & 2 & 2 \\
0 & 2 & 5 
\end{array}\rightB$
\exitem $A = \leftB \begin{array}{rrr}
3 & 0 & 7 \\
0 & 5 & 0 \\
7 & 0 & 3 
\end{array}\rightB$
\exitem $A = \leftB \begin{array}{rrr}
1 & 1 & 0 \\
1 & 1 & 0 \\
0 & 0 & 2 
\end{array}\rightB$
\exitem $A = \leftB \begin{array}{rrr}
5 & -2 & -4 \\
-2 & 8 & -2\\
-4 & -2 & 5 
\end{array}\rightB$
\exitem* $A = \leftB \begin{array}{rrrr}
5 & 3 & 0 & 0 \\
3 & 5 & 0 & 0 \\
0 & 0 & 7 & 1 \\
0 & 0 & 1 & 7 
\end{array}\rightB$
\exitem* $A = \leftB \begin{array}{rrrr}
3 & 5 & -1 & 1 \\
5 & 3 & 1 & -1 \\
-1 & 1 & 3 & 5 \\
1 & -1 & 5 & 3 
\end{array}\rightB$
\end{exenumerate}
\begin{sol}
\begin{enumerate}[label={\alph*.}]
\setcounter{enumi}{1}
\item  $\frac{1}{\sqrt{2}}\leftB \begin{array}{rr}
1 & -1 \\
1 & 1
\end{array}\rightB$

\setcounter{enumi}{3}
\item  $\frac{1}{\sqrt{2}}\leftB \begin{array}{rrr}
0 & 1 & 1\\
\sqrt{2} & 0 & 0\\
0 & 1 & -1
\end{array}\rightB$

\setcounter{enumi}{5}
\item  $\frac{1}{3\sqrt{2}}\leftB \begin{array}{rrr}
2\sqrt{2} & 3 & 1\\
\sqrt{2} & 0 & -4\\
2\sqrt{2} & -3 & 1
\end{array}\rightB$ or $\frac{1}{3}\leftB \begin{array}{rrr}
2 & -2 & 1\\
1 & 2 & 2\\
2 & 1 & -2
\end{array}\rightB$

\setcounter{enumi}{7}
\item  $\frac{1}{2}\leftB \begin{array}{rrrr}
1 & -1 & \sqrt{2} & 0\\
-1 & 1 & \sqrt{2} & 0\\
-1 & -1 & 0 & \sqrt{2}\\
1 & 1 & 0 & \sqrt{2}
\end{array}\rightB$

\end{enumerate}
\end{sol}
\end{ex}

\begin{ex}
Consider $A = \leftB \begin{array}{rrr}
0 & a & 0 \\
a & 0 & c \\
0 & c & 0 
\end{array}\rightB$
 where one of $a, c \neq 0$. Show that $c_{A}(x) = x(x - k)(x + k)$, where $k = \sqrt{a^2 + c^2}$ and find an orthogonal matrix $P$ such that $P^{-1}AP$ is diagonal.

\begin{sol}
$P = \frac{1}{\sqrt{2}k}\leftB \begin{array}{rrr}
c\sqrt{2} & a & a \\
0 & k & -k \\
-a\sqrt{2} & c & c
\end{array}\rightB$
\end{sol}
\end{ex}

\begin{ex}
Consider $A = \leftB \begin{array}{rrr}
0 & 0 & a \\
0 & b & 0 \\
a & 0 & 0 
\end{array}\rightB$. Show that $c_{A}(x) = (x - b)(x - a)(x + a)$ and find an orthogonal matrix $P$ such that $P^{-1}AP$ is diagonal.
\end{ex}

\begin{ex}
Given $A = \leftB \begin{array}{rr}
b & a \\
a & b
\end{array}\rightB$, show that \\ $c_{A}(x) = (x - a - b)(x + a - b)$ and find an orthogonal matrix $P$ such that $P^{-1}AP$ is diagonal.
\end{ex}

\begin{ex}
Consider $A = \leftB \begin{array}{rrr}
b & 0 & a \\
0 & b & 0 \\
a & 0 & b 
\end{array}\rightB$. Show that $c_{A}(x) = (x - b)(x - b - a)(x - b + a)$ and find an orthogonal matrix $P$ such that $P^{-1}AP$ is diagonal.
\end{ex}

\begin{ex}
In each case find new variables $y_{1}$ and $y_{2}$ that diagonalize the quadratic form $q$.

\begin{exenumerate}
\exitem $q = x_{1}^2 + 6x_{1}x_2 + x_{2}^2$
\exitem $q = x_{1}^2 + 4x_{1}x_2 - 2x_{2}^2$
\end{exenumerate}
\begin{sol}
\begin{enumerate}[label={\alph*.}]
\setcounter{enumi}{1}
\item  $y_{1} = \frac{1}{\sqrt{5}}(-x_{1} + 2x_{2})$ and $y_{2} = \frac{1}{\sqrt{5}}(2x_{1} + x_{2})$; $q = -3y_{1}^2 + 2y_{2}^2$.


\end{enumerate}
\end{sol}
\end{ex}

\begin{ex}
Show that the following are equivalent for a symmetric matrix $A$.


\begin{exenumerate}
\exitem $A$ is orthogonal.
\exitem $A^{2} = I$.
\exitem* All eigenvalues of $A$ are $\pm 1$.
\end{exenumerate}
[Hint: For (b) if and only if (c), use Theorem~\ref{thm:024303}.]


\begin{sol}
\begin{enumerate}[label={\alph*.}]
\setcounter{enumi}{2}
\item $\Rightarrow$ a. By Theorem~\ref{thm:024227} let $P^{-1}AP = D = \func{diag}(\lambda_{1}, \dots, \lambda_{n})$ where the $\lambda_{i}$ are the eigenvalues of $A$. By c. we have $\lambda_{i} = \pm 1$ for each $i$, whence $D^{2} = I$. But then $A^{2} = (PDP^{-1})^{2} = PD^{2}P^{-1} = I$. Since $A$ is symmetric this is $AA^{T} = I$, proving a.

\end{enumerate}
\end{sol}
\end{ex}


\begin{ex}\label{ex:8_2_12}
We call matrices $A$ and $B$ \textbf{orthogonally similar}\index{orthogonality!orthogonally similar}\index{orthogonally similar} (and write $A \stackrel{\circ}{\sim} B$) if $B = P^{T}AP$ for an orthogonal matrix $P$.


\begin{enumerate}[label={\alph*.}]
\item Show that $A \stackrel{\circ}{\sim} A$ for all $A$; $A \stackrel{\circ}{\sim} B \Rightarrow B \stackrel{\circ}{\sim} A$; and $A \stackrel{\circ}{\sim} B$ and $B \stackrel{\circ}{\sim} C \Rightarrow A \stackrel{\circ}{\sim} C$.

\item Show that the following are equivalent for two symmetric matrices $A$ and $B$.


\begin{enumerate}[label={\roman*.}]
\item $A$ and $B$ are similar.

\item $A$ and $B$ are orthogonally similar.

\item $A$ and $B$ have the same eigenvalues.

\end{enumerate}
\end{enumerate}
\end{ex}

\begin{ex}
Assume that $A$ and $B$ are orthogonally similar (Exercise \ref{ex:8_2_12}).


\begin{enumerate}[label={\alph*.}]
\item If $A$ and $B$ are invertible, show that $A^{-1}$ and $B^{-1}$ are orthogonally similar.

\item Show that $A^{2}$ and $B^{2}$ are orthogonally similar.

\item Show that, if $A$ is symmetric, so is $B$.

\end{enumerate}
\begin{sol}
\begin{enumerate}[label={\alph*.}]
\setcounter{enumi}{1}
\item  If $B = P^{T}AP = P^{-1}$, then $B^{2} = P^{T}APP^{T}AP = P^{T}A^{2}P$.

\end{enumerate}
\end{sol}
\end{ex}

\begin{ex}
If $A$ is symmetric, show that every eigenvalue of $A$ is nonnegative if and only if $A = B^{2}$ for some symmetric matrix $B$.
\end{ex}

\begin{ex}\label{ex:8_2_15}
Prove the converse of Theorem~\ref{thm:024396}:


If $(A\vect{x}) \dotprod \vect{y} = \vect{x} \dotprod (A\vect{y})$ for all $n$-columns $\vect{x}$ and $\vect{y}$, then $A$ is symmetric.

\begin{sol}
If $\vect{x}$ and $\vect{y}$ are respectively columns $i$ and $j$ of $I_{n}$, then $\vect{x}^{T}A^{T}\vect{y} = \vect{x}^{T}A\vect{y}$ shows that the $(i, j)$-entries of $A^{T}$ and $A$ are equal.
\end{sol}
\end{ex}

\begin{ex}
Show that every eigenvalue of $A$ is zero if and only if $A$ is nilpotent ($A^{k} = 0$ for some $k \geq 1$).
\end{ex}

\begin{ex}
If $A$ has real eigenvalues, show that $A = B + C$ where $B$ is symmetric and $C$ is nilpotent. \newline [\textit{Hint}: Theorem~\ref{thm:024503}.]
\end{ex}

\begin{ex}
Let $P$ be an orthogonal matrix.


\begin{enumerate}[label={\alph*.}]
\item Show that $\func{det}P = 1$ or $\func{det}P = -1$.

\item Give $2 \times 2$ examples of $P$ such that $\func{det}P = 1$ and $\func{det} P = -1$.

\item If $\func{det} P = -1$, show that $I + P$ has no inverse. [\textit{Hint}: $P^{T}(I + P) = (I + P)^{T}$.]

\item If $P$ is $n \times n$ and $\func{det}P \neq (-1)^{n}$, show that $I - P$ has no inverse.


[\textit{Hint}: $P^{T}(I - P) = -(I - P)^{T}$.]

\end{enumerate}
\begin{sol} 
\begin{enumerate}[label={\alph*.}]
\setcounter{enumi}{1}
\item $\func{det}\leftB \begin{array}{rr}
\cos\theta & -\sin\theta \\
\sin\theta & \cos\theta
\end{array}\rightB = 1$ \\ and $\func{det}\leftB \begin{array}{rr}
\cos\theta & \sin\theta \\
\sin\theta & -\cos\theta
\end{array}\rightB = -1$

[\textit{Remark}: These are the \textit{only} $2 \times 2$ examples.]

\setcounter{enumi}{3}
\item Use the fact that $P^{-1} = P^{T}$ to show that $P^{T}(I - P) = -(I - P)^{T}$. Now take determinants and use the hypothesis that $\func{det}P \neq (-1)^{n}$.

\end{enumerate}
\end{sol}
\end{ex}

\begin{ex}
We call a square matrix $E$ a \textbf{projection matrix}\index{matrix!projection matrix}\index{projection matrix} if $E^{2} = E = E^{T}$. (See Exercise \ref{ex:8_1_17}.)


\begin{enumerate}[label={\alph*.}]
\item If $E$ is a projection matrix, show that $P = I - 2E$ is orthogonal and symmetric.

\item If $P$ is orthogonal and symmetric, show that \\ $E = \frac{1}{2}(I - P)$ is a projection matrix.

\item If $U$ is $m \times n$ and $U^{T}U = I$ (for example, a unit column in $\RR^n$), show that $E = UU^{T}$ is a projection matrix.

\end{enumerate}
\end{ex}

\begin{ex}
A matrix that we obtain from the identity matrix by writing its rows in a different order is called a \textbf{permutation matrix}\index{matrix!permutation matrix}\index{permutation matrix}. Show that every permutation matrix is orthogonal.
\end{ex}

\begin{ex}
If the rows $\vect{r}_{1}, \dots, \vect{r}_{n}$ of the $n \times n$ matrix $A = \leftB a_{ij} \rightB$ are orthogonal, show that the $(i, j)$-entry of $A^{-1}$ is $\frac{a_{ji}}{\vectlength \vect{r}_{j} \vectlength^2}$.

\begin{sol}
We have $AA^{T} = D$, where $D$ is diagonal with main diagonal entries $\vectlength R_{1}\vectlength^{2}, \dots, \vectlength R_{n}\vectlength^{2}$. Hence $A^{-1} = A^{T}D^{-1}$, and the result follows because $D^{-1}$ has diagonal entries $1 / \vectlength R_{1}\vectlength^{2}, \dots, 1 / \vectlength R_{n}\vectlength^{2}$.
\end{sol}
\end{ex}

\begin{ex}
\begin{enumerate}[label={\alph*.}]
\item Let $A$ be an $m \times n$ matrix. Show that the following are equivalent.


\begin{enumerate}[label={\roman*.}]
\item $A$ has orthogonal rows.

\item $A$ can be factored as $A = DP$, where $D$ is invertible and diagonal and $P$ has orthonormal rows.

\item $AA^{T}$ is an invertible, diagonal matrix.

\end{enumerate}
\item Show that an $n \times n$ matrix $A$ has orthogonal rows if and only if $A$ can be factored as $A = DP$, where $P$ is orthogonal and $D$ is diagonal and invertible.

\end{enumerate}
\end{ex}

\begin{ex}
Let $A$ be a skew-symmetric matrix; that is, $A^{T} = -A$. Assume that $A$ is an $n \times n$ matrix.


\begin{enumerate}[label={\alph*.}]
\item Show that $I + A$ is invertible. [\textit{Hint}: By Theorem~\ref{thm:004553}, it suffices to show that $(I + A)\vect{x} = \vect{0}$, $\vect{x}$ in $\RR^n$, implies $\vect{x} = \vect{0}$. Compute $\vect{x} \dotprod \vect{x} = \vect{x}^{T}\vect{x}$, and use the fact that $A\vect{x} = -\vect{x}$ and $A^{2}\vect{x} = \vect{x}$.]

\item Show that $P = (I - A)(I + A)^{-1}$ is orthogonal.

\item Show that every orthogonal matrix $P$ such that $I + P$ is invertible arises as in part (b) from some skew-symmetric matrix $A$. \newline [\textit{Hint}: Solve $P = (I - A)(I + A)^{-1}$ for $A$.]

\end{enumerate}
\begin{sol}
\begin{enumerate}[label={\alph*.}]
\setcounter{enumi}{1}
\item  Because $I - A$ and $I + A$ commute, $PP^{T} = (I - A)(I + A)^{-1}[(I + A)^{-1}]^{T}(I - A)^{T} = (I - A)(I + A)^{-1}(I - A)^{-1}(I + A) = I$.

\end{enumerate}
\end{sol}
\end{ex}

\begin{ex}
Show that the following are equivalent for an $n \times n$ matrix $P$.


\begin{enumerate}[label={\alph*.}]
\item $P$ is orthogonal.

\item $\vectlength P\vect{x}\vectlength = \vectlength\vect{x}\vectlength$ for all columns $\vect{x}$ in $\RR^n$.

\item $\vectlength P\vect{x} - P\vect{y}\vectlength = \vectlength\vect{x} - \vect{y}\vectlength$ for all columns $\vect{x}$ and $\vect{y}$ in $\RR^n$.

\item $(P\vect{x}) \dotprod (P\vect{y}) = \vect{x} \dotprod \vect{y}$ for all columns $\vect{x}$ and $\vect{y}$ in $\RR^n$.


[\textit{Hints}: For (c) $\Rightarrow$ (d), see Exercise \ref{ex:5_3_14}(a). For (d) $\Rightarrow$ (a), show that column $i$ of $P$ equals $P\vect{e}_{i}$, where $\vect{e}_{i}$ is column $i$ of the identity matrix.]

\end{enumerate}
\end{ex}



\begin{ex}
Show that every $2 \times 2$ orthogonal matrix has the form $\leftB \begin{array}{rr}
\cos\theta & -\sin\theta \\
\sin\theta & \cos\theta
\end{array}\rightB$ or $\leftB \begin{array}{rr}
\cos\theta & \sin\theta \\
\sin\theta & -\cos\theta
\end{array}\rightB$
 for some angle $\theta$. \newline [\textit{Hint}: If $a^{2} + b^{2} = 1$, then $a = \cos\theta$ and $b = \sin\theta$ for some angle $\theta$.]
\end{ex}

\begin{ex}
Use Theorem~\ref{thm:024503} to show that every symmetric matrix is orthogonally diagonalizable.
\end{ex}
\end{multicols}
