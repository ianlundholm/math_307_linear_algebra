\section{Elementary Matrices}
\label{sec:2_5}

It is now clear that elementary row operations are important in linear algebra: They are essential in solving linear systems (using the gaussian algorithm) and in inverting a matrix (using the matrix inversion algorithm). It turns out that they can be performed by left multiplying by certain invertible matrices. These matrices are the subject of this section.

\begin{definition}{Elementary Matrices}{005200}
An $n \times n$ matrix $E$ is called an \textbf{elementary matrix}\index{elementary matrix!defined}\index{matrix!elementary matrix}\index{square matrix ($n \times n$ matrix)!elementary matrix} if it can be obtained from the identity matrix $I_{n}$ by a single elementary row operation (called the operation \textbf{corresponding} to $E$)\index{elementary matrix!operating corresponding to}\index{elementary row operations!corresponding}\index{matrix algebra!elementary matrix}. We say that $E$ is of type I, II, or III if the operation is of that type (see Definition \ref{def:000795}).
\end{definition}

Hence
\begin{equation*}
E_{1} = \leftB \begin{array}{rr}
0 & 1 \\
1 & 0
\end{array} \rightB, \quad E_{2} = \leftB \begin{array}{rr}
1 & 0 \\
0 & 9
\end{array} \rightB, \quad \mbox{ and } \quad E_{3} = \leftB \begin{array}{rr}
1 & 5 \\
0 & 1
\end{array} \rightB
\end{equation*}
are elementary of types I, II, and III, respectively, obtained from the $2 \times 2$ identity matrix by interchanging rows 1 and 2, multiplying row 2 by
 $9$, and adding $5$ times row 2 to row 1.

Suppose now that the matrix $A = \leftB \begin{array}{ccc}
a & b & c \\
p & q & r
\end{array} \rightB$
 is left multiplied by the above elementary matrices $E_{1}$, $E_{2}$, and $E_{3}$. The results are:
\begin{align*}
E_{1}A &= \leftB \begin{array}{rr}
0 & 1 \\
1 & 0
\end{array} \rightB \leftB \begin{array}{rrr}
a & b & c \\
p & q & r
\end{array} \rightB = \leftB \begin{array}{rrr}
p & q & r \\
a & b & c
\end{array} \rightB \\
E_{2}A &= \leftB \begin{array}{rr}
1 & 0 \\
0 & 9
\end{array} \rightB \leftB \begin{array}{rrr}
a & b & c \\
p & q & r
\end{array} \rightB = \leftB \begin{array}{ccc}
a & b & c \\
9p & 9q & 9r
\end{array} \rightB \\
E_{3}A &= \leftB \begin{array}{rr}
1 & 5 \\
0 & 1
\end{array} \rightB \leftB \begin{array}{ccc}
a & b & c \\
p & q & r
\end{array} \rightB = \leftB \begin{array}{ccc}
a + 5p & b + 5q & c + 5r \\
p & q & r
\end{array} \rightB
\end{align*}
In each case, left multiplying $A$ by the elementary matrix has the \textit{same} effect as doing the corresponding row operation to $A$. This works in general.

\begin{lemma}{\footnotemark}{005213}
If an elementary row operation is performed on an $m \times n$ matrix $A$, the result is $EA$ where $E$ is the elementary matrix obtained by performing the same operation on the $m \times m$ identity matrix.\index{$m \times n$ matrix!elementary row operation}
\end{lemma} 
\footnotetext{A \textit{lemma}\index{lemma} is an auxiliary theorem used in the proof of other theorems.\index{auxiliary theorem}}

\begin{proof}
We prove it for operations of type III; the proofs for types I and II are left as exercises. Let $E$ be the elementary matrix corresponding to the operation that adds $k$ times row $p$ to row $q \neq p$. The proof depends on the fact that each row of $EA$ is equal to the corresponding row of $E$ times $A$. Let $K_{1}, K_{2}, \dots, K_{m}$ denote the rows of $I_{m}$. Then row $i$ of $E$ is $K_{i}$ if $i \neq q$, while row $q$ of $E$ is $K_{q} + kK_{p}$. Hence:
\begin{alignat*}{3}
& \mbox{If } i \neq q \mbox{ then row } i \mbox{ of } EA =&&K_{i}A  &&= (\mbox{row } i \mbox{ of } A). \\
& \mbox{Row } q \mbox{ of } EA  =  (K_{q} + kK_{p})A  && = && K_{q}A + k(K_{p}A) \\
&		 && = && (\mbox{row } q \mbox{ of } A) \mbox{ plus } k\ (\mbox{row } p \mbox{ of } A).
\end{alignat*}
Thus $EA$ is the result of adding $k$ times row $p$ of $A$ to row $q$, as required.
\end{proof}

The effect of an elementary row operation can be reversed by another such operation (called its inverse) which is also elementary of the same type (see the discussion following (Example~\ref{exa:000809}). It follows that each elementary matrix $E$ is invertible. In fact, if a row operation on $I$ produces $E$, then the inverse operation carries $E$ back to $I$\index{elementary row operations!inverses}\index{inverses!elementary row operations}. If $F$ is the elementary matrix corresponding to the inverse operation, this means $FE = I$ (by Lemma~\ref{lem:005213}). Thus $F = E^{-1}$ and we have proved

\begin{lemma}{}{005237}
Every elementary matrix $E$ is invertible, and $E^{-1}$ is also a elementary matrix (of the same type). Moreover, $E^{-1}$ corresponds to the inverse of the row operation that produces $E$.
\end{lemma}

\noindent The following table gives the inverse of each type of elementary row operation:\index{elementary matrix!and inverses}\index{inverses!and elementary matrices}

\begin{table}[H]
	\centering
	\begin{tabu}{|c|c|c|}
	\hline
	\textbf{Type} & \textbf{Operation} & \textbf{Inverse Operation} \\ \hline
	I & Interchange rows $p$ and $q$ & Interchange rows $p$ and $q$ \\
	II & Multiply row $p$ by $k \neq 0$ & Multiply row $p$ by $1/k$, $k \neq 0$ \\
	III & Add $k$ times row $p$ to row $q \neq p$ & Subtract $k$ times row $p$ from row $q$, $q \neq p$ \\ \hline
	\end{tabu}
\end{table}
\noindent Note that elementary matrices of type I are self-inverse.\index{elementary matrix!self-inverse}

\begin{example}{}{005264}
Find the inverse of each of the elementary matrices
\begin{equation*}
E_{1} = \leftB \begin{array}{rrr}
0 & 1 & 0 \\
1 & 0 & 0 \\
0 & 0 & 1
\end{array} \rightB, \quad
E_{2} = \leftB \begin{array}{rrr}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 9
\end{array} \rightB, \quad \mbox{and} \quad
E_{3} = \leftB \begin{array}{rrr}
1 & 0 & 5 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array} \rightB.
\end{equation*}
\begin{solution}
  $E_{1}$, $E_{2}$, and $E_{3}$ are of type I, II, and III respectively, so the table gives
\begin{equation*}
E_{1}^{-1} = \leftB \begin{array}{rrr}
0 & 1 & 0 \\
1 & 0 & 0 \\
0 & 0 & 1
\end{array} \rightB = E_{1}, \quad
E_{2}^{-1} = \leftB \begin{array}{rrr}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & \frac{1}{9}
\end{array} \rightB, \quad \mbox{and} \quad
E_{3}^{-1} = \leftB \begin{array}{rrr}
1 & 0 & -5 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array} \rightB.
\end{equation*}
\end{solution}
\end{example}

\subsection*{Inverses and Elementary Matrices}

Suppose that an $m \times n$ matrix $A$ is carried to a matrix $B$ (written $A \to B$) by a series of $k$ elementary row operations. Let $E_{1}, E_{2}, \dots, E_{k}$ denote the corresponding elementary matrices. By Lemma~\ref{lem:005213}, the reduction becomes
\begin{equation*}
A \rightarrow E_{1}A \rightarrow E_{2}E_{1}A \rightarrow E_{3}E_{2}E_{1}A \rightarrow \cdots \rightarrow E_{k}E_{k-1} \cdots E_{2}E_{1}A = B
\end{equation*}
In other words,
\begin{equation*}
A \rightarrow UA = B \quad \mbox{ where } U = E_{k}E_{k-1} \cdots E_{2}E_{1}
\end{equation*}\index{invertible matrix!product of elementary matrix}
\noindent The matrix $U = E_{k}E_{k-1} \cdots E_{2}E_{1}$ is invertible, being a product of invertible matrices by Lemma~\ref{lem:005237}. Moreover, $U$ can be computed without finding the $E_{i}$ as follows: If the above series of operations carrying $A \to B$ is performed on $I_{m}$ in place of $A$, the result is $I_{m} \to UI_{m} = U$. Hence this series of operations carries the block matrix $\leftB \begin{array}{cc}
A & I_{m}
\end{array} \rightB \to \leftB \begin{array}{cc}
B & U
\end{array} \rightB$. This, together with the above discussion, proves

\begin{theorem}{}{005294}
Suppose $A$ is $m \times n$ and $A \to B$ by elementary row operations.

\begin{enumerate}
\item $B = UA$ where $U$ is an $m \times m$ invertible matrix.

\item $U$ can be computed by $\leftB \begin{array}{cc}
A & I_{m}
\end{array} \rightB \to \leftB \begin{array}{cc}
B & U
\end{array} \rightB$ using the operations carrying $A \to B$.

\item $U = E_{k}E_{k-1} \cdots E_{2}E_{1}$ where $E_{1}, E_{2}, \dots, E_{k}$ are the elementary matrices corresponding (in order) to the elementary row operations carrying $A$ to $B$.

\end{enumerate}
\end{theorem}

\begin{example}{}{005312}
If $A = \leftB \begin{array}{rrr}
2 & 3 & 1 \\
1 & 2 & 1
\end{array} \rightB$, express the reduced row-echelon form $R$ of $A$ as $R = UA$ where $U$ is invertible.

\begin{solution}
  Reduce the double matrix $\leftB \begin{array}{cc}
  A & I
  \end{array} \rightB \rightarrow \leftB \begin{array}{cc}
  R & U
  \end{array} \rightB$ as follows:
\begin{align*}
\leftB \begin{array}{cc}
A & I
\end{array} \rightB = \leftB \begin{array}{rrr|rr}
2 & 3 & 1 & 1 & 0 \\
1 & 2 & 1 & 0 & 1
\end{array} \rightB \rightarrow
\leftB \begin{array}{rrr|rr}
1 & 2 & 1 & 0 & 1 \\
2 & 3 & 1 & 1 & 0
\end{array} \rightB &\rightarrow
\leftB \begin{array}{rrr|rr}
1 & 2 & 1 & 0 & 1 \\
0 & -1 & -1 & 1 & -2
\end{array} \rightB \\
&\rightarrow
\leftB \begin{array}{rrr|rr}
1 & 0 & -1 & 2 & -3 \\
0 & 1 & 1 & -1 & 2
\end{array} \rightB
\end{align*}
Hence $R = \leftB \begin{array}{rrr}
1 & 0 & -1 \\
0 & 1 & 1
\end{array} \rightB$
 and $U = \leftB \begin{array}{rr}
 	2 & -3 \\
 	-1 & 2
 \end{array} \rightB$.
\end{solution}
\end{example}

Now suppose that $A$ is invertible. We know that $A \to I$ by Theorem~\ref{thm:004553}, so taking $B = I$ in Theorem~\ref{thm:005294} gives $\leftB \begin{array}{cc}
A & I
\end{array} \rightB \rightarrow \leftB \begin{array}{cc}
I & U
\end{array} \rightB$ where $I = UA$. Thus $U = A^{-1}$, so we have $\leftB \begin{array}{cc}
A & I
\end{array} \rightB \rightarrow \leftB \begin{array}{cc}
I & A^{-1}
\end{array} \rightB$. This is the matrix inversion algorithm\index{matrix inversion algorithm} in Section~\ref{sec:2_4}. However, more is true: Theorem~\ref{thm:005294} gives $A^{-1} = U = E_{k}E_{k-1} \cdots E_{2}E_{1}$ where $E_{1}, E_{2}, \dots, E_{k}$ are the elementary matrices corresponding (in order) to the row operations carrying $A \to I$. Hence
\begin{equation}
A = \left(A^{-1}\right)^{-1} = \left(E_{k}E_{k-1} \cdots E_{2}E_{1}\right)^{-1} = E_{1}^{-1}E_{2}^{-1} \cdots E_{k-1}^{-1}E_{k}^{-1}
\end{equation}
By Lemma~\ref{lem:005237}, this shows that every invertible matrix $A$ is a product of elementary matrices. Since elementary matrices are invertible (again by Lemma~\ref{lem:005237}), this proves the following important characterization of invertible matrices.

\begin{theorem}{}{005336}
A square matrix is invertible if and only if it is a product of elementary matrices.
\end{theorem}

It follows from Theorem \ref{thm:005294} that $A \to B$ by row operations if and only if $B = UA$ for some invertible matrix $U$. In this case we say that $A$ and $B$ are \textbf{row-equivalent}. (See Exercise~\ref{ex:ex2_5_17}.)

\begin{example}{}{005340}
Express $A = \leftB \begin{array}{rr}
-2 & 3 \\
1 & 0
\end{array} \rightB$
 as a product of elementary matrices.

\begin{solution}
  Using Lemma~\ref{lem:005213}, the reduction of $A \to I$ is as follows:
\begin{equation*}
A = \leftB \begin{array}{rr}
-2 & 3 \\
1 & 0
\end{array} \rightB \rightarrow 
E_{1}A = \leftB \begin{array}{rr}
1 & 0 \\
-2 & 3
\end{array} \rightB \rightarrow
E_{2}E_{1}A = \leftB \begin{array}{rr}
1 & 0 \\
0 & 3
\end{array} \rightB \rightarrow
E_{3}E_{2}E_{1}A = \leftB \begin{array}{rr}
1 & 0 \\
0 & 1
\end{array} \rightB
\end{equation*}
where the corresponding elementary matrices are
\begin{equation*}
E_{1} = \leftB \begin{array}{rr}
0 & 1 \\
1 & 0
\end{array} \rightB, \quad
E_{2} = \leftB \begin{array}{rr}
1 & 0 \\
2 & 1
\end{array} \rightB, \quad
E_{3} = \leftB \begin{array}{rr}
1 & 0 \\
0 & \frac{1}{3}
\end{array} \rightB
\end{equation*}
Hence $(E_{3}\ E_{2}\ E_{1})A = I$, so:
\begin{equation*}
A = \left(E_{3}E_{2}E_{1}\right)^{-1} = E_{1}^{-1}E_{2}^{-1}E_{3}^{-1} = \leftB \begin{array}{rr}
0 & 1 \\
1 & 0
\end{array} \rightB \leftB \begin{array}{rr}
1 & 0 \\
-2 & 1
\end{array} \rightB \leftB \begin{array}{rr}
1 & 0 \\
0 & 3
\end{array} \rightB
\end{equation*}
\end{solution}
\end{example}

\subsection*{Smith Normal Form}

Let $A$ be an $m \times n$ matrix of $\func{rank }r$, and let $R$ be the reduced row-echelon form of $A$. Theorem~\ref{thm:005294} shows that $R = UA$ where $U$ is invertible, and that $U$ can be found from $\leftB \begin{array}{cc}
A & I_{m}
\end{array} \rightB \rightarrow \leftB \begin{array}{cc}
R & U
\end{array} \rightB$.

The matrix $R$ has $r$ leading ones (since $\func{rank }A = r$) so, as $R$ is reduced, the $n \times m$ matrix $R^{T}$ contains each row of $I_{r}$ in the first $r$ columns. Thus row operations will carry $R^{T} \rightarrow \leftB \begin{array}{cc}
I_{r} & 0 \\
0 & 0
\end{array} \rightB_{n \times m}$. Hence Theorem~\ref{thm:005294} (again) shows that $\leftB \begin{array}{cc}
 I_{r} & 0 \\
 0 & 0
 \end{array} \rightB_{n \times m} = U_{1}R^{T}$
 where $U_{1}$ is an $n \times n$ invertible matrix. Writing $V = U_{1}^{T}$, we obtain
\begin{equation*}
UAV = RV = RU_{1}^{T} = \left(U_{1}R^{T}\right)^{T} = \left( \leftB \begin{array}{cc}
I_{r} & 0 \\
0 & 0
\end{array} \rightB_{n \times m} \right)^{T} = \leftB \begin{array}{cc}
I_{r} & 0 \\
0 & 0
\end{array} \rightB_{m \times n}
\end{equation*}
Moreover, the matrix $U_{1} = V^{T}$ can be computed by $\leftB \begin{array}{cc}
R^{T} & I_{n}
\end{array} \rightB \rightarrow \leftB \leftB \begin{array}{cc}
I_{r} & 0 \\
0 & 0
\end{array} \rightB_{n \times m} V^{T} \rightB$.
 This proves

\begin{theorem}{}{005369}
Let $A$ be an $m \times n$ matrix of $\func{rank }r$. There exist invertible matrices $U$ and $V$ of size $m \times m$ and $n \times n$, respectively, such that
\begin{equation*}
UAV = \leftB \begin{array}{cc}
I_{r} & 0 \\
0 & 0
\end{array} \rightB_{m \times n}
\end{equation*}
Moreover, if $R$ is the reduced row-echelon form of $A$, then:

\begin{enumerate}
\item $U$ can be computed by $\leftB \begin{array}{cc}
A & I_{m}
\end{array} \rightB \rightarrow \leftB \begin{array}{cc}
R & U
\end{array} \rightB$;

\item $V$ can be computed by $\leftB \begin{array}{cc}
R^{T} & I_{n}
\end{array} \rightB \rightarrow \leftB \leftB \begin{array}{cc}
I_{r} & 0 \\
0 & 0
\end{array} \rightB_{n \times m} V^{T} \rightB$.

\end{enumerate}
\end{theorem}

If $A$ is an $m \times n$ matrix of $\func{rank }r$, the matrix 
$\leftB \begin{array}{cc}
I_{r} & 0 \\
0 & 0
\end{array} \rightB$
 is called the \textbf{Smith normal form}\index{Smith normal form}\index{columns!Smith normal form}\index{elementary matrix!Smith normal form}\index{rows!Smith normal form}\footnote{Named after Henry John Stephen Smith (1826--83).} of $A$. Whereas the reduced row-echelon form of $A$ is the ``nicest'' matrix to which $A$ can be carried by row operations, the Smith canonical form is the ``nicest'' matrix to which $A$ can be carried by \textit{row and column} operations. This is because doing row operations to $R^{T}$ amounts to doing \textit{column} operations to $R$ and then transposing.

\begin{example}{}{005384}
Given $A = \leftB \begin{array}{rrrr}
1 & -1 & 1 & 2 \\
2 & -2 & 1 & -1 \\
-1 & 1 & 0 & 3
\end{array} \rightB$, find invertible matrices $U$ and $V$ such that $UAV = \leftB \begin{array}{cc}
 I_{r} & 0 \\
 0 & 0
 \end{array} \rightB$, where $r = \func{rank }A$.

\begin{solution}
  The matrix $U$ and the reduced row-echelon form $R$ of $A$ are computed by the row reduction 
  $\leftB \begin{array}{cc}
  A & I_{3}
  \end{array} \rightB \rightarrow \leftB \begin{array}{cc}
  R & U
  \end{array} \rightB$:
\begin{equation*}
\leftB \begin{array}{rrrr|rrr}
1 & -1 & 1 & 2 & 1 & 0 & 0 \\
2 & -2 & 1 & -1 & 0 & 1 & 0 \\
-1 & 1 & 0 & 3 & 0 & 0 & 1
\end{array} \rightB \rightarrow
\leftB \begin{array}{rrrr|rrr}
1 & -1 & 0 & -3 & -1 & 1 & 0 \\
0 & 0 & 1 & 5 & 2 & -1 & 0 \\
0 & 0 & 0 & 0 & -1 & 1 & 1
\end{array} \rightB
\end{equation*}
Hence
\begin{equation*}
R = \leftB \begin{array}{rrrr}
1 & -1 & 0 & -3 \\
0 & 0 & 1 & 5 \\
0 & 0 & 0 & 0
\end{array} \rightB \quad \mbox{ and } \quad
U = \leftB \begin{array}{rrr}
-1 & 1 & 0 \\
2 & -1 & 0 \\
-1 & 1 & 1
\end{array} \rightB
\end{equation*}
In particular, $r = \func{rank }R = 2$. Now row-reduce $\leftB \begin{array}{cc}
R^{T} & I_{4}
\end{array} \rightB \rightarrow \leftB \begin{array}{cc}
\leftB \begin{array}{cc}
I_{r} & 0 \\
0 & 0
\end{array} \rightB & V^{T}
\end{array} \rightB$:
\begin{equation*}
\leftB \begin{array}{rrr|rrrr}
1 & 0 & 0 & 1 & 0 & 0 & 0 \\
-1 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 & 1 & 0 \\
-3 & 5 & 0 & 0 & 0 & 0 & 1
\end{array} \rightB \rightarrow 
\leftB \begin{array}{rrr|rrrr}
1 & 0 & 0 & 1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 & 1 & 0 & 0 \\
0 & 0 & 0 & 3 & 0 & -5 & 1
\end{array} \rightB
\end{equation*}
whence
\begin{equation*}
V^{T} = \leftB \begin{array}{rrrr}
1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 \\
1 & 1 & 0 & 0 \\
3 & 0 & -5 & -1
\end{array} \rightB \quad \mbox { so } \quad
V = \leftB \begin{array}{rrrr}
1 & 0 & 1 & 3 \\
0 & 0 & 1 & 0 \\
0 & 1 & 0 & -5 \\
0 & 0 & 0 & 1
\end{array} \rightB
\end{equation*}
Then $UAV = \leftB \begin{array}{cc}
I_{2} & 0 \\
0 & 0
\end{array} \rightB$
 as is easily verified.
\end{solution}
\end{example}

\subsection*{Uniqueness of the Reduced Row-echelon Form}

In this short subsection, Theorem~\ref{thm:005294} is used to prove the following important theorem.\index{elementary matrix!uniqueness of reduced row-echelon form}\index{matrix!reduced row-echelon matrix}\index{matrix form!reduced row-echelon form}\index{reduced row-echelon form}

\begin{theorem}{}{005405}
If a matrix $A$ is carried to reduced row-echelon matrices $R$ and $S$ by row operations, then $R = S$.
\end{theorem}

\begin{proof}
Observe first that $UR = S$ for some invertible matrix $U$ (by Theorem~\ref{thm:005294} there exist invertible matrices $P$ and $Q$ such that $R = PA$ and $S = QA$; take $U = QP^{-1}$). We show that $R = S$ by induction on the number $m$ of rows of $R$ and $S$. The case $m = 1$ is left to the reader. If $R_{j}$ and $S_{j}$ denote column $j$ in $R$ and $S$ respectively, the fact that $UR = S$ gives
\begin{equation} \label{eq:urs}
UR_{j} = S_{j} \quad \mbox{ for each } j
\end{equation}
Since $U$ is invertible, this shows that $R$ and $S$ have the same zero columns. Hence, by passing to the matrices obtained by deleting the zero columns from $R$ and $S$, we may assume that $R$ and $S$ have no zero columns.

But then the first column of $R$ and $S$ is the first column of $I_{m}$ because $R$ and $S$ are row-echelon, so (\ref{eq:urs}) shows that the first column of $U$ is column 1 of $I_{m}$. Now write $U$, $R$, and $S$ in block form as follows.
\begin{equation*}
U = \leftB \begin{array}{cc}
1 & X \\
0 & V
\end{array} \rightB, \quad
R = \leftB \begin{array}{cc}
1 & Y \\
0 & R^\prime
\end{array} \rightB, \quad \mbox{ and } \quad
S = \leftB \begin{array}{cc}
1 & Z \\
0 & S^\prime
\end{array} \rightB
\end{equation*}
Since $UR = S$, block multiplication gives $VR^\prime = S^\prime$ so, since $V$ is invertible ($U$ is invertible) and both $R^\prime$ and $S^\prime$ are reduced row-echelon, we obtain $R^\prime = S^\prime$ by induction. Hence $R$ and $S$ have the same number (say $r$) of leading $1$s, and so both have $m$--$r$ zero rows.

In fact, $R$ and $S$ have leading ones in the same columns, say $r$ of them. Applying (\ref{eq:urs}) to these columns shows that the first $r$ columns of $U$ are the first $r$ columns of $I_{m}$. Hence we can write $U$, $R$, and $S$ in block form as follows:
\begin{equation*}
U = \leftB \begin{array}{cc}
I_{r} & M \\
0 & W
\end{array} \rightB, \quad
R = \leftB \begin{array}{cc}
R_{1} & R_{2} \\
0 & 0
\end{array} \rightB, \quad \mbox{ and } \quad
S = \leftB \begin{array}{cc}
S_{1} & S_{2} \\
0 & 0
\end{array} \rightB
\end{equation*}
where $R_{1}$ and $S_{1}$ are $r \times r$. Then using $UR=S$ block multiplication gives $R_{1} = S_{1}$ and $R_{2} = S_{2}$; that is, $S = R$. This completes the proof.
\end{proof}
