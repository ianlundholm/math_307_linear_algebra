\section{Positive Definite Matrices}
\label{sec:8_3}\index{orthogonality!positive definite matrix}\index{positive definite matrix}\index{square matrix ($n \times n$ matrix)!positive definite matrix}\index{symmetric matrix!positive definite}

All the eigenvalues of any symmetric 
matrix are real; this section is about the case in which the eigenvalues
 are positive. These matrices, which arise whenever optimization 
(maximum and minimum) problems are encountered, have countless 
applications throughout science and engineering. They also arise in 
statistics (for example, in factor analysis used in the social sciences)
 and in geometry (see Section~\ref{sec:8_8}). We will encounter them again in Chapter~\ref{chap:10} when describing all inner products in $\RR^n$.


\begin{definition}{Positive Definite Matrices}{024811}
A square matrix is called \textbf{positive definite}\index{positive definite} if it is symmetric and all its eigenvalues $\lambda$ are positive, that is $\lambda > 0$.
\end{definition}

Because these matrices are symmetric, the principal axes theorem plays a central role in the theory.


\begin{theorem}{}{024815}
If $A$ is positive definite, then it is invertible and $\func{det}A > 0$.
\end{theorem}

\begin{proof}
If $A$ is $n \times n$ and the eigenvalues are $\lambda_{1}, \lambda_{2}, \dots, \lambda_{n}$, then $\func{det}A = \lambda_{1}\lambda_{2} \cdots \lambda_{n} > 0$ by the principal axes theorem (or the corollary to Theorem~\ref{thm:024503}).
\end{proof}

If $\vect{x}$ is a column in $\RR^n$ and $A$ is any real $n \times n$ matrix, we view the $1 \times 1$ matrix $\vect{x}^{T}A\vect{x}$ as a real number. With this convention, we have the following characterization of positive definite matrices.


\begin{theorem}{}{024830}
A symmetric matrix $A$ is positive definite if and only if $\vect{x}^{T} A \vect{x} > 0$ for every column $\vect{x} \neq \vect{0}$ in $\RR^n$.
\end{theorem}

\begin{proof}
$A$ is symmetric so, by the principal axes theorem, let $P^{T}AP = D = \func{diag}(\lambda_{1}, \lambda_{2}, \dots, \lambda_{n})$ where $P^{-1} = P^{T}$ and the $\lambda_{i}$ are the eigenvalues of $A$. Given a column $\vect{x}$ in $\RR^n$, write $\vect{y} = P^{T}\vect{x} = \leftB \begin{array}{cccc}
y_{1} & y_{2} & \dots & y_{n}
\end{array}\rightB^T$. Then
\begin{equation} \label{symmetricEq}
\vect{x}^TA\vect{x} = \vect{x}^T(PDP^T)\vect{x} = \vect{y}^TD\vect{y} = \lambda_{1}y_{1}^2 + \lambda_{2}y_{2}^2 + \dots + \lambda_{n}y_{n}^2
\end{equation}
If $A$ is positive definite and $\vect{x} \neq \vect{0}$, then $\vect{x}^{T}A\vect{x} > 0$ by (\ref{symmetricEq}) because some $y_{j} \neq 0$ and every $\lambda_{i} > 0$. Conversely, if $\vect{x}^{T}A\vect{x} > 0$ whenever $\vect{x} \neq \vect{0}$, let $\vect{x} = P\vect{e}_{j} \neq \vect{0}$ where $\vect{e}_{j}$ is column $j$ of $I_{n}$. Then $\vect{y} = \vect{e}_{j}$, so (\ref{symmetricEq}) reads $\lambda_{j} = \vect{x}^{T}A\vect{x} > 0$.
\end{proof}

\noindent Note that Theorem~\ref{thm:024830} shows that the positive definite matrices are exactly the symmetric matrices $A$ for which the quadratic form $q = \vect{x}^{T}A\vect{x}$ takes only positive values.


\begin{example}{}{024865}
If $U$ is any invertible $n \times n$ matrix, show that $A = U^{T}U$ is positive definite.


\begin{solution}
  If $\vect{x}$ is in $\RR^n$ and $\vect{x} \neq \vect{0}$, then
\begin{equation*}
\vect{x}^TA\vect{x} =  \vect{x}^T(U^TU)\vect{x} = (U\vect{x})^T(U\vect{x})= \vectlength U\vect{x} \vectlength^2 > 0
\end{equation*}
because $U\vect{x} \neq \vect{0}$ ($U$ is invertible). Hence Theorem~\ref{thm:024830} applies.
\end{solution}
\end{example}

It is remarkable that the converse to Example~\ref{exa:024865} is also true. In fact every positive definite matrix $A$ can be factored as $A = U^{T}U$ where $U$ is an upper triangular matrix with positive elements on the main 
diagonal. However, before verifying this, we introduce another concept 
that is central to any discussion of positive definite matrices.


If $A$ is any $n \times n$ matrix, let $^{(r)}A$ denote the $r \times r$ submatrix in the upper left corner of $A$; that is, $^{(r)}A$ is the matrix obtained from $A$ by deleting the last $n - r$ rows and columns. The matrices $^{(1)}A, ^{(2)}A, ^{(3)}A, \dots$, $^{(n)}A = A$ are called the \textbf{principal submatrices}\index{principal submatrices} of $A$.


\begin{example}{}{024883}
If $A = \leftB \begin{array}{rrr}
10 & 5 & 2 \\
5 & 3 & 2 \\
2 & 2 & 3
\end{array}\rightB$ then $^{(1)}A = \leftB 10 \rightB$, $^{(2)}A = \leftB \begin{array}{rr}
10 & 5 \\
5 & 3
\end{array}\rightB$ and $^{(3)}A = A$.
\end{example}

\begin{lemma}{}{024890}
If $A$ is positive definite, so is each principal submatrix $^{(r)}A$ for $r = 1, 2, \dots, n$.
\end{lemma}

\begin{proof}
Write $A = \leftB \begin{array}{rr}
^{(r)}A & P \\
Q & R
\end{array}\rightB$
 in block form. If $\vect{y} \neq \vect{0}$ in $\RR^r$, write $\vect{x} = \leftB \begin{array}{r}
 \vect{y} \\
 \vect{0}
 \end{array}\rightB$ in $\RR^n$.

Then $\vect{x} \neq \vect{0}$, so the fact that $A$ is positive definite gives
\begin{equation*}
0 < \vect{x}^TA\vect{x} = \leftB \begin{array}{rr}
\vect{y}^T & \vect{0}
\end{array}\rightB \leftB \begin{array}{rr}
^{(r)}A & P \\
Q & R
\end{array}\rightB \leftB \begin{array}{r}
\vect{y} \\
\vect{0}
\end{array}\rightB = \vect{y}^T(^{(r)}A)\vect{y}
\end{equation*}
This shows that $^{(r)}A$ is positive definite by Theorem~\ref{thm:024830}.\footnote{A similar argument shows that, if $B$ is any matrix obtained from a positive definite matrix $A$ by deleting certain rows and deleting the \textit{same} columns, then $B$ is also positive definite.}
\end{proof}

If $A$ is positive definite, Lemma~\ref{lem:024890} and Theorem~\ref{thm:024815} show that $\func{det}(^{(r)}A) > 0$ for every $r$. This proves part of the following theorem which contains the converse to Example~\ref{exa:024865}, and characterizes the positive definite matrices among the symmetric ones.


\begin{theorem}{}{024907}
The following conditions are equivalent for a symmetric $n \times n$ matrix $A$:

\vspace*{-1em}
\begin{enumerate}
\item $A$ is positive definite.

\item $\func{det}(^{(r)}A) > 0$ for each $r = 1, 2, \dots, n$.

\item $A = U^{T}U$ where $U$ is an upper triangular matrix with positive entries on the main diagonal.
\end{enumerate}

Furthermore, the factorization in (3) is unique (called the \textbf{Cholesky factorization}\footnotemark of $A$).\index{Cholesky factorization}
\end{theorem}
\footnotetext{Andre-Louis
 Cholesky\index{Cholesky, Andre-Louis} (1875--1918), was a French mathematician who died in World War 
I. His factorization was published in 1924 by a fellow officer.}

\begin{proof}
First, (3) $\Rightarrow$ (1) by Example~\ref{exa:024865}, and (1) $\Rightarrow$ (2) by Lemma~\ref{lem:024890} and Theorem~\ref{thm:024815}. 

(2) $\Rightarrow$ (3). Assume (2) and proceed by induction on $n$. If $n = 1$, then $A = \leftB a \rightB$ where $a > 0$ by (2), so take $U = \leftB \sqrt{a} \rightB$. If $n > 1$, write $B =^{(n-1)}A$. Then $B$ is symmetric and satisfies (2) so, by induction, we have $B = U^{T}U$ as in (3) where $U$ is of size $(n - 1) \times (n - 1)$. Then, as $A$ is symmetric, it has block form $A = \leftB \begin{array}{cc}
B & \vect{p} \\
\vect{p}^T & b
\end{array}\rightB$ where $\vect{p}$ is a column in $\RR^{n-1}$ and $b$ is in $\RR$. If we write $\vect{x} = (U^{T})^{-1}\vect{p}$ and $c = b - \vect{x}^{T}\vect{x}$, block multiplication gives
\begin{equation*}
A = \leftB \begin{array}{cc}
U^TU & \vect{p} \\
\vect{p}^T & b
\end{array}\rightB = \leftB \begin{array}{cc}
U^T & 0 \\
\vect{x}^T & 1
\end{array}\rightB \leftB \begin{array}{cc}
U & \vect{x} \\
0 & c
\end{array}\rightB
\end{equation*}
as the reader can verify. Taking determinants and applying Theorem~\ref{thm:007890} gives $\func{det}A = \func{det}(U^{T}) \func{det} U \cdot c = c(\func{det}U)^{2}$. Hence $c > 0$ because $\func{det}A > 0$ by (2), so the above factorization can be written 
\begin{equation*}
A = \leftB \begin{array}{cc}
U^T & 0 \\
\vect{x}^T & \sqrt{c}
\end{array}\rightB \leftB \begin{array}{cc}
U & \vect{x} \\
0 & \sqrt{c}
\end{array}\rightB
\end{equation*}
Since $U$ has positive diagonal entries, this proves (3).

As to the uniqueness, suppose that $A = U^TU = U_{1}^TU_{1}$ are two Cholesky factorizations. Now write $D = UU_{1}^{-1} = (U^T)^{-1}U_{1}^T$. Then $D$ is upper triangular, because $D = UU_{1}^{-1}$, and lower triangular, because $D = (U^T)^{-1}U_{1}^T$, and so it is a diagonal matrix. Thus $U = DU_{1}$ and $U_{1} = DU$, so it suffices to show that $D = I$. But eliminating $U_{1}$ gives $U = D^{2}U$, so $D^{2} = I$ because $U$ is invertible. Since the diagonal entries of $D$ are positive (this is true of $U$ and $U_{1}$), it follows that $D = I$.
\end{proof}

The remarkable thing is that the matrix $U$ in the Cholesky factorization is easy to obtain from $A$ using row operations. The key is that Step 1 of the following algorithm is \textit{possible} for any positive definite matrix $A$. A proof of the algorithm is given following Example~\ref{exa:024959}.


\begin{theorem*}[label=thm:024947]{Algorithm for the Cholesky Factorization}
If $A$ is a positive definite matrix, the Cholesky factorization $A = U^{T}U$ can be obtained as follows:


\begin{itemize}[leftmargin=4em]
\item[Step 1.] Carry $A$ to an upper triangular matrix $U_{1}$ with positive diagonal entries using row operations each of which adds a multiple of a row to a lower row.

\item[Step 2.] Obtain $U$ from $U_{1}$ by dividing each row of $U_{1}$ by the square root of the diagonal entry in that row.

\end{itemize}\index{Cholesky algorithm}
\end{theorem*}

\begin{example}{}{024959}
Find the Cholesky factorization of $A = \leftB \begin{array}{rrr}
10 & 5 & 2 \\
5 & 3 & 2 \\
2 & 2 & 3
\end{array}\rightB$.


\begin{solution}
  The matrix $A$ is positive definite by Theorem~\ref{thm:024907} because $\func{det}{^{(1)}A} = 10 > 0$, $\func{det}{^{(2)}A} = 5 > 0$, and $\func{det}{^{(3)}A} = \func{det} A = 3 > 0$. Hence Step 1 of the algorithm is carried out as follows:
\begin{equation*}
A = \leftB \begin{array}{rrr}
10 & 5 & 2 \\
5 & 3 & 2 \\
2 & 2 & 3
\end{array}\rightB \rightarrow \leftB \begin{array}{rrc}
10 & 5 & 2 \\
0 & \frac{1}{2} & 1 \\
0 & 1 & \frac{13}{5}
\end{array}\rightB \rightarrow \leftB \begin{array}{rrr}
10 & 5 & 2 \\
0 & \frac{1}{2} & 1 \\
0 & 0 & \frac{3}{5}
\end{array}\rightB = U_{1}
\end{equation*}
Now carry out Step 2 on $U_{1}$ to obtain $U = \leftB \def\arraystretch{1.5}\begin{array}{ccc}
\sqrt{10} & \frac{5}{\sqrt{10}} & \frac{2}{\sqrt{10}} \\
0 & \frac{1}{\sqrt{2}} & \sqrt{2} \\
0 & 0 & \frac{\sqrt{3}}{\sqrt{5}}
\end{array}\rightB$.

The reader can verify that $U^{T}U = A$.
\end{solution}
\end{example}

\begin{proof}[Proof of the Cholesky Algorithm]
If $A$ is positive definite, let $A = U^{T}U$ be the Cholesky factorization, and let $D = \func{diag}(d_{1}, \dots, d_{n})$ be the common diagonal of $U$ and $U^{T}$. Then $U^{T}D^{-1}$ is lower triangular with ones on the diagonal (call such matrices LT-1). Hence $L = (U^{T}D^{-1})^{-1}$ is also LT-1, and so $I_{n} \to L$ by a sequence of row operations each of which adds a multiple of a row 
to a lower row (verify; modify columns right to left). But then $A \to LA$ by the same sequence of row operations (see the discussion preceding Theorem~\ref{thm:005294}). Since $LA = [D(U^{T})^{-1}][U^{T}U] = DU$ is upper triangular with positive entries on the diagonal, this shows that Step 1 of the algorithm is possible.

Turning to Step 2, let $A \to U_{1}$ as in Step 1 so that $U_{1} = L_{1}A$ where $L_{1}$ is
LT-1. Since A is symmetric, we get
\begin{equation} \label{symmetricEq2}
L_{1}U_{1}^T = L_{1}(L_{1}A)^T = L_{1}A^TL_{1}^T = L_{1}AL_{1}^T = U_{1}L_{1}^T
\end{equation}
Let $D_{1} = \func{diag}(e_{1}, \dots, e_{n})$ denote the diagonal of $U_{1}$. Then (\ref{symmetricEq2}) gives $L_{1}(U_{1}^TD_{1}^{-1}) = U_{1}L_{1}^TD_{1}^{-1}$. This is both upper triangular (right side) and LT-1 (left side), and so must equal $I_{n}$. In particular, $U_{1}^TD_{1}^{-1} = L_{1}^{-1}$. Now let $D_{2} = \func{diag}(\sqrt{e_{1}}, \dots, \sqrt{e_{n}})$, so that $D_{2}^2 = D_{1}$. If we write $U = D_{2}^{-1}U_{1}$
 we have
\begin{equation*}
U^TU = (U_{1}^TD_{2}^{-1})(D_{2}^{-1}U_{1}) = U_{1}^T(D_{2}^2)^{-1}U_{1} = (U_{1}^TD_{1}^{-1})U_{1} = (L_{1}^{-1})U_{1} = A
\end{equation*}
This proves Step 2 because $U = D_{2}^{-1}U_{1}$ is formed by dividing each row of $U_{1}$ by the square root of its diagonal entry (verify).
\end{proof}
